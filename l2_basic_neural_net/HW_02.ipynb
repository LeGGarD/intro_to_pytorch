{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15f0a399f10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='materials/data/',\n",
    "                                             train=True,  \n",
    "                                             transform=transforms.ToTensor(), \n",
    "                                             download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                            train=False,\n",
    "                                            download=True, \n",
    "                                            transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=64,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    \"\"\" \n",
    "    The basic perceptron. \n",
    "    A fully connected layer, which takes input, multiplies it by the weights matrix, \n",
    "    adds bias and uses an activation function.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim: int, \n",
    "        output_dim: int, \n",
    "        bias: bool = True,\n",
    "        activation: callable = torch.sigmoid\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.bias = bias\n",
    "        self.activation_func = activation\n",
    "        # Use the class method to initialize the weights.\n",
    "        self.fc = self._init_weights(nn.Linear(input_dim, output_dim, bias=bias))        \n",
    "        \n",
    "    def _init_weights(self, layer):\n",
    "        \"\"\"\n",
    "        I decided to initialize weights with the Xavier's algorithm here, \n",
    "        since we are using the Sigmoid activation function\n",
    "        \n",
    "        In the case of ReLU I would switch the initializer to the \"Kaiman's\" a.k.a \"He\".\n",
    "        \"\"\"\n",
    "        layer.weight.data = nn.init.xavier_uniform_(\n",
    "            layer.weight.data, \n",
    "            gain=nn.init.calculate_gain(self.activation_func.__name__)\n",
    "        )\n",
    "        return layer\n",
    "        \n",
    "    def forward(self, input):\n",
    "        fc_out = self.fc(input)\n",
    "        output = self.activation_func(fc_out)\n",
    "        return output\n",
    "        \n",
    "\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A very simple model with 4 hidden layers.\n",
    "    Each layer is a Perceptron with different number of units.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = Perceptron(input_shape, hidden_dim * 5)\n",
    "        self.fc2 = Perceptron(hidden_dim * 5, hidden_dim * 4)\n",
    "        self.fc3 = Perceptron(hidden_dim * 4, hidden_dim * 3)\n",
    "        self.fc4 = Perceptron(hidden_dim * 3, hidden_dim * 2)\n",
    "        self.fc5 = Perceptron(hidden_dim * 2, hidden_dim)\n",
    "        self.fc6 = Perceptron(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # As far as the model is going to take various images as the input:\n",
    "        # input.shape -> (n_images, n_channels, width, height) \n",
    "        # We have to flatten the incoming parts of the tensor, which are actually the images. \n",
    "        input = torch.flatten(input, start_dim=1)        \n",
    "        # input = input.view(input.shape[0], -1)\n",
    "        layer_1 = self.fc1.forward(input)\n",
    "        layer_2 = self.fc2.forward(layer_1)\n",
    "        layer_3 = self.fc3.forward(layer_2)\n",
    "        layer_4 = self.fc4.forward(layer_3)\n",
    "        layer_5 = self.fc5.forward(layer_4)\n",
    "        layer_6 = self.fc6.forward(layer_5)\n",
    "        return layer_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5124, -0.0262,  0.5933,  0.6523,  0.2227, -0.1314,  0.2060,  0.6567,\n",
       "         -0.2944,  0.0543],\n",
       "        [ 0.2842, -0.2615,  0.0070, -0.3143, -0.5217,  0.5771, -0.2221, -0.0046,\n",
       "          0.5049,  0.4867]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test how the torch function works\n",
    "lin = nn.Linear(10, 2, bias=True)\n",
    "torch.nn.init.xavier_uniform_(lin.weight.data, gain=nn.init.calculate_gain('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weights:\n",
      "tensor([[ 0.0060, -0.0932,  0.9977,  0.4943],\n",
      "        [-0.7330,  1.0627,  0.7955, -0.0533]])\n",
      "\n",
      "Standard Deviation:\n",
      "0.631\n"
     ]
    }
   ],
   "source": [
    "# Test how the torch function works\n",
    "lin = nn.Linear(4, 2, bias=True)\n",
    "weights = torch.nn.init.kaiming_normal_(lin.weight.data)\n",
    "\n",
    "print(f'Initialized weights:\\n{weights}\\n\\nStandard Deviation:\\n{weights.std():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, the loss and the optimizer.\n",
    "model = SimpleLinearModel(3072, 2, 10)\n",
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0302, -0.0117,  0.0426,  ..., -0.0244,  0.0253,  0.0214],\n",
      "        [ 0.0047,  0.0376,  0.0014,  ...,  0.0277,  0.0013, -0.0027],\n",
      "        [-0.0272,  0.0380,  0.0402,  ..., -0.0210, -0.0312, -0.0022],\n",
      "        ...,\n",
      "        [ 0.0308,  0.0068, -0.0423,  ...,  0.0277, -0.0342, -0.0340],\n",
      "        [ 0.0299, -0.0118,  0.0103,  ...,  0.0123,  0.0302,  0.0278],\n",
      "        [ 0.0398,  0.0367,  0.0303,  ..., -0.0111, -0.0050, -0.0173]])\n",
      "tensor([-0.0003, -0.0099, -0.0041,  0.0140, -0.0152, -0.0067,  0.0128, -0.0176,\n",
      "         0.0055,  0.0006])\n",
      "tensor([[-0.1157,  0.1383, -0.1043, -0.4139, -0.5422,  0.4862, -0.1357,  0.0311,\n",
      "          0.4555,  0.1627],\n",
      "        [-0.4905, -0.4141, -0.0296, -0.2748, -0.3856,  0.4967, -0.1463,  0.2099,\n",
      "         -0.2512, -0.3669],\n",
      "        [ 0.0922, -0.4889,  0.0728,  0.4196, -0.2611,  0.4141,  0.0128,  0.3254,\n",
      "         -0.2208,  0.2289],\n",
      "        [-0.3754,  0.5252, -0.5391,  0.1381, -0.4574,  0.2102,  0.2640, -0.2687,\n",
      "         -0.3638,  0.5699],\n",
      "        [-0.5668,  0.4594,  0.4930, -0.1892, -0.1920, -0.1801, -0.5598, -0.4364,\n",
      "         -0.3649, -0.0695],\n",
      "        [-0.1843, -0.5540, -0.3371,  0.0716, -0.0921,  0.2537,  0.3992,  0.4877,\n",
      "         -0.3351,  0.2204],\n",
      "        [-0.5485, -0.3534,  0.5620,  0.5756,  0.3007,  0.3852,  0.0446,  0.5617,\n",
      "         -0.0593, -0.2476],\n",
      "        [ 0.0619,  0.5085, -0.3305,  0.3221,  0.5437,  0.5077, -0.5696, -0.2192,\n",
      "          0.3557,  0.1784]])\n",
      "tensor([-0.2293, -0.2101, -0.0357, -0.2782, -0.0249, -0.2568,  0.0433,  0.3073])\n",
      "tensor([[-0.2311, -0.3623,  0.6279,  0.5803, -0.6017,  0.1476,  0.5634,  0.0837],\n",
      "        [ 0.2213, -0.1746,  0.5151, -0.0484,  0.0423, -0.1292,  0.0357, -0.2995],\n",
      "        [-0.3446, -0.0242,  0.1646, -0.2120, -0.0667, -0.2185,  0.2936, -0.5570],\n",
      "        [-0.0752,  0.1665,  0.5871,  0.1978, -0.5778, -0.1660, -0.4910, -0.4470],\n",
      "        [ 0.3273,  0.0038,  0.2759,  0.3535, -0.2236, -0.5778, -0.6241, -0.2661],\n",
      "        [-0.4031,  0.1897, -0.2568,  0.3049,  0.2205,  0.2594,  0.5104, -0.0728]])\n",
      "tensor([ 0.1360,  0.1494, -0.2535, -0.1050, -0.1747,  0.2659])\n",
      "tensor([[-0.5653,  0.5835,  0.2585, -0.1387,  0.2783, -0.6696],\n",
      "        [-0.5470, -0.2132, -0.3711,  0.4841, -0.0215,  0.3411],\n",
      "        [-0.5407, -0.4251,  0.5874, -0.6340,  0.2754,  0.7038],\n",
      "        [ 0.2030,  0.2691,  0.1436, -0.7460, -0.0196, -0.6976]])\n",
      "tensor([ 0.2442,  0.2737,  0.2853, -0.1410])\n",
      "tensor([[-0.5488, -0.6035, -0.0538, -0.8029],\n",
      "        [ 0.9099, -0.8648, -0.8990,  0.5414]])\n",
      "tensor([ 0.4399, -0.3084])\n",
      "tensor([[ 0.0215,  0.6416],\n",
      "        [-0.5233, -0.2646],\n",
      "        [-0.5207,  0.7000],\n",
      "        [ 0.4861,  0.2983],\n",
      "        [-0.0909, -0.4694],\n",
      "        [ 0.2435, -0.6857],\n",
      "        [-0.6211,  0.2370],\n",
      "        [ 0.6559,  0.3798],\n",
      "        [ 0.5322, -0.4882],\n",
      "        [ 0.6113, -0.4856]])\n",
      "tensor([-0.6835, -0.6062, -0.0817, -0.0074,  0.0884,  0.1748, -0.2662, -0.1772,\n",
      "         0.1112,  0.3462])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), 6)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a few sample images\n",
    "image1, label1 = train_dataset[0]\n",
    "image2, label2 = train_dataset[1]\n",
    "image3, label3 = train_dataset[2]\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making just one back-propagation step to test the model\n",
    "inputs = torch.cat((image1[None, :], image2[None, :], image3[None, :]))\n",
    "labels = torch.tensor([label1, label2, label3])\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels.view(-1))\n",
    "loss.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      "tensor([[0.1040, 0.1022, 0.1153, 0.0936, 0.1084, 0.0914, 0.0857, 0.0864, 0.1077,\n",
      "         0.1052],\n",
      "        [0.1039, 0.1022, 0.1153, 0.0935, 0.1083, 0.0915, 0.0857, 0.0865, 0.1078,\n",
      "         0.1052],\n",
      "        [0.1038, 0.1021, 0.1154, 0.0934, 0.1083, 0.0916, 0.0858, 0.0865, 0.1079,\n",
      "         0.1052]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "sum of predictions validity:\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# Checking the softmax predictions\n",
    "outputs_sum = outputs.sum()\n",
    "print(f'predictions:\\n{outputs}\\n\\nsum of predictions validity:\\n{outputs_sum:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputted softmax value:\n",
      "[0.0857, 0.10524, 0.10523]\n",
      "\n",
      "Loss (-log(predict)):\n",
      "[2.18179, 2.18179, 2.18179]\n",
      "\n",
      "Calculated Torch Loss:\n",
      "tensor([2.3169, 2.2974, 2.2974], grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Checking the Cross Entropy Loss \n",
    "outputted_softmax_prediction_values = []\n",
    "manual_losses = []\n",
    "for i, sample in enumerate(outputs):\n",
    "    outputted_softmax_prediction_values.append(round(float(sample[labels[i]]), 5))\n",
    "    manual_losses.append(round(-np.log(softmax_value), 5))\n",
    "    \n",
    "print(f'Outputted softmax value:\\n{outputted_softmax_prediction_values}\\n\\nLoss (-log(predict)):\\n{manual_losses}\\n\\nCalculated Torch Loss:\\n{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.9453e-05,  2.8776e-05,  2.6059e-05,  ...,  2.5201e-06,\n",
      "          9.6958e-06,  1.1444e-05],\n",
      "        [ 1.3771e-05,  1.3440e-05,  1.2171e-05,  ...,  1.2703e-06,\n",
      "          4.5900e-06,  5.3997e-06],\n",
      "        [ 1.3196e-05,  1.1626e-05,  1.1353e-05,  ...,  1.2328e-05,\n",
      "          9.8528e-06,  9.3848e-06],\n",
      "        ...,\n",
      "        [ 2.8472e-06,  3.0299e-06,  2.5371e-06,  ..., -2.1129e-06,\n",
      "         -1.6859e-07,  2.7622e-07],\n",
      "        [ 6.2058e-05,  5.8297e-05,  5.4431e-05,  ...,  2.6368e-05,\n",
      "          3.0647e-05,  3.1944e-05],\n",
      "        [ 1.2753e-05,  1.2280e-05,  1.1081e-05,  ...,  2.1173e-06,\n",
      "          4.8875e-06,  5.5732e-06]])\n",
      "tensor([ 1.4175e-05,  6.7701e-06,  2.5010e-05,  2.4019e-05, -2.0729e-05,\n",
      "        -2.6079e-06,  6.5089e-05,  7.6672e-06])\n",
      "tensor([[ 8.1050e-05,  1.0620e-04,  4.8572e-05,  7.0846e-05,  9.8121e-05,\n",
      "          6.6954e-05,  8.0300e-05,  1.0778e-04],\n",
      "        [ 5.3314e-05,  7.8954e-05,  6.4523e-05,  4.1612e-05,  8.4868e-05,\n",
      "          3.5887e-05,  8.5015e-05,  7.7863e-05],\n",
      "        [ 1.6305e-04,  2.1082e-04,  8.9219e-05,  1.4412e-04,  1.9251e-04,\n",
      "          1.3609e-04,  1.5316e-04,  2.1473e-04],\n",
      "        [-1.2736e-04, -1.6860e-04, -8.3950e-05, -1.1042e-04, -1.5931e-04,\n",
      "         -1.0265e-04, -1.3373e-04, -1.7075e-04]])\n",
      "tensor([ 0.0002,  0.0001,  0.0003, -0.0002])\n",
      "tensor([[-1.6172e-03, -6.4958e-04, -1.5947e-03, -1.1672e-03],\n",
      "        [ 4.3008e-04,  4.6371e-05,  4.9225e-04,  3.4121e-04]])\n",
      "tensor([-0.0023,  0.0006])\n",
      "tensor([[ 0.0027,  0.0024],\n",
      "        [ 0.0027,  0.0024],\n",
      "        [ 0.0026,  0.0023],\n",
      "        [ 0.0026,  0.0023],\n",
      "        [ 0.0027,  0.0024],\n",
      "        [ 0.0025,  0.0022],\n",
      "        [-0.0054, -0.0048],\n",
      "        [ 0.0023,  0.0021],\n",
      "        [ 0.0027,  0.0024],\n",
      "        [-0.0157, -0.0141]])\n",
      "tensor([ 0.0073,  0.0073,  0.0071,  0.0069,  0.0073,  0.0067, -0.0148,  0.0062,\n",
      "         0.0073, -0.0420])\n"
     ]
    }
   ],
   "source": [
    "# Check that the gradients were calculated\n",
    "for param in model.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, batch 1] loss: 2.3480\n",
      "[Epoch 1, batch 101] loss: 2.3091\n",
      "[Epoch 1, batch 201] loss: 2.3020\n",
      "[Epoch 1, batch 301] loss: 2.2855\n",
      "[Epoch 1, batch 401] loss: 2.2666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:47<07:08, 47.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2, batch 1] loss: 2.1689\n",
      "[Epoch 2, batch 101] loss: 2.1680\n",
      "[Epoch 2, batch 201] loss: 2.1603\n",
      "[Epoch 2, batch 301] loss: 2.1556\n",
      "[Epoch 2, batch 401] loss: 2.1514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [01:30<05:56, 44.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3, batch 1] loss: 2.1694\n",
      "[Epoch 3, batch 101] loss: 2.1301\n",
      "[Epoch 3, batch 201] loss: 2.1261\n",
      "[Epoch 3, batch 301] loss: 2.1236\n",
      "[Epoch 3, batch 401] loss: 2.1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [01:44<03:35, 30.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4, batch 1] loss: 2.0786\n",
      "[Epoch 4, batch 101] loss: 2.1118\n",
      "[Epoch 4, batch 201] loss: 2.1099\n",
      "[Epoch 4, batch 301] loss: 2.1088\n",
      "[Epoch 4, batch 401] loss: 2.1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [01:58<02:24, 24.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5, batch 1] loss: 2.1582\n",
      "[Epoch 5, batch 101] loss: 2.0957\n",
      "[Epoch 5, batch 201] loss: 2.0959\n",
      "[Epoch 5, batch 301] loss: 2.0959\n",
      "[Epoch 5, batch 401] loss: 2.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [02:12<01:42, 20.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6, batch 1] loss: 2.1444\n",
      "[Epoch 6, batch 101] loss: 2.0872\n",
      "[Epoch 6, batch 201] loss: 2.0879\n",
      "[Epoch 6, batch 301] loss: 2.0890\n",
      "[Epoch 6, batch 401] loss: 2.0888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [02:28<01:15, 18.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7, batch 1] loss: 2.0483\n",
      "[Epoch 7, batch 101] loss: 2.0836\n",
      "[Epoch 7, batch 201] loss: 2.0822\n",
      "[Epoch 7, batch 301] loss: 2.0835\n",
      "[Epoch 7, batch 401] loss: 2.0830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [02:41<00:51, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8, batch 1] loss: 2.1275\n",
      "[Epoch 8, batch 101] loss: 2.0782\n",
      "[Epoch 8, batch 201] loss: 2.0777\n",
      "[Epoch 8, batch 301] loss: 2.0773\n",
      "[Epoch 8, batch 401] loss: 2.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [02:56<00:32, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9, batch 1] loss: 2.0879\n",
      "[Epoch 9, batch 101] loss: 2.0732\n",
      "[Epoch 9, batch 201] loss: 2.0753\n",
      "[Epoch 9, batch 301] loss: 2.0756\n",
      "[Epoch 9, batch 401] loss: 2.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [03:11<00:15, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10, batch 1] loss: 2.0590\n",
      "[Epoch 10, batch 101] loss: 2.0731\n",
      "[Epoch 10, batch 201] loss: 2.0704\n",
      "[Epoch 10, batch 301] loss: 2.0719\n",
      "[Epoch 10, batch 401] loss: 2.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [03:25<00:00, 20.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's train the model for a few epochs using the visualization library tqdm \n",
    "# and the train_loader, which allows us to train the model each epoch on a new random batch.\n",
    "\n",
    "hidden_layer_dim = 100\n",
    "model = SimpleLinearModel(3072, hidden_layer_dim, 10)\n",
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):  \n",
    "    current_loss = 0.0\n",
    "    running_items = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss.mean().item()\n",
    "        running_items += len(labels)\n",
    "        if i % 100 == 0:\n",
    "            print(f'[Epoch {epoch + 1}, batch {i + 1}] loss: {current_loss / (running_items / 100):.4f}')\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, batch 1] loss: 2.1306\n",
      "[Epoch 1, batch 101] loss: 2.0673\n",
      "[Epoch 1, batch 201] loss: 2.0649\n",
      "[Epoch 1, batch 301] loss: 2.0641\n",
      "[Epoch 1, batch 401] loss: 2.0636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:14<02:09, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2, batch 1] loss: 2.0675\n",
      "[Epoch 2, batch 101] loss: 2.0633\n",
      "[Epoch 2, batch 201] loss: 2.0642\n",
      "[Epoch 2, batch 301] loss: 2.0639\n",
      "[Epoch 2, batch 401] loss: 2.0631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [00:28<01:52, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3, batch 1] loss: 2.0581\n",
      "[Epoch 3, batch 101] loss: 2.0700\n",
      "[Epoch 3, batch 201] loss: 2.0652\n",
      "[Epoch 3, batch 301] loss: 2.0643\n",
      "[Epoch 3, batch 401] loss: 2.0620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [00:42<01:37, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4, batch 1] loss: 2.0728\n",
      "[Epoch 4, batch 101] loss: 2.0579\n",
      "[Epoch 4, batch 201] loss: 2.0585\n",
      "[Epoch 4, batch 301] loss: 2.0600\n",
      "[Epoch 4, batch 401] loss: 2.0605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:55<01:23, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5, batch 1] loss: 2.0754\n",
      "[Epoch 5, batch 101] loss: 2.0670\n",
      "[Epoch 5, batch 201] loss: 2.0613\n",
      "[Epoch 5, batch 301] loss: 2.0611\n",
      "[Epoch 5, batch 401] loss: 2.0606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [01:09<01:09, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6, batch 1] loss: 2.0664\n",
      "[Epoch 6, batch 101] loss: 2.0612\n",
      "[Epoch 6, batch 201] loss: 2.0599\n",
      "[Epoch 6, batch 301] loss: 2.0602\n",
      "[Epoch 6, batch 401] loss: 2.0592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [01:23<00:55, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7, batch 1] loss: 2.0436\n",
      "[Epoch 7, batch 101] loss: 2.0610\n",
      "[Epoch 7, batch 201] loss: 2.0590\n",
      "[Epoch 7, batch 301] loss: 2.0581\n",
      "[Epoch 7, batch 401] loss: 2.0581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [01:38<00:42, 14.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8, batch 1] loss: 2.0642\n",
      "[Epoch 8, batch 101] loss: 2.0612\n",
      "[Epoch 8, batch 201] loss: 2.0601\n",
      "[Epoch 8, batch 301] loss: 2.0590\n",
      "[Epoch 8, batch 401] loss: 2.0575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [01:54<00:29, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9, batch 1] loss: 2.1084\n",
      "[Epoch 9, batch 101] loss: 2.0537\n",
      "[Epoch 9, batch 201] loss: 2.0547\n",
      "[Epoch 9, batch 301] loss: 2.0567\n",
      "[Epoch 9, batch 401] loss: 2.0574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [02:10<00:15, 15.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10, batch 1] loss: 2.0360\n",
      "[Epoch 10, batch 101] loss: 2.0549\n",
      "[Epoch 10, batch 201] loss: 2.0535\n",
      "[Epoch 10, batch 301] loss: 2.0550\n",
      "[Epoch 10, batch 401] loss: 2.0557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [02:24<00:00, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):  \n",
    "    current_loss = 0.0\n",
    "    running_items = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss.mean().item()\n",
    "        running_items += len(labels)\n",
    "        if i % 100 == 0:\n",
    "            print(f'[Epoch {epoch + 1}, batch {i + 1}] loss: {current_loss / (running_items / 100):.4f}')\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score isn't good one here, because it wasn't allowed to use CNNs in this homework. I could also change the score to MSE, it should probably work better with sigmoid activations. And changing the activations to ReLU could also make the model better. \n",
    "\n",
    "However, the key point here was that the model trains at all :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10_pytorch",
   "language": "python",
   "name": "python3.10_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
